// Package src provides benchmark integration with the optimized client stack.
// This module extends the benchmark system to work with HTTP/2, caching, and monitoring.
package main

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"net/http"
	"os"
	"sync"
	"time"
)

// IntegratedBenchmarkConfig extends BenchmarkConfig with optimization options
type IntegratedBenchmarkConfig struct {
	// Base benchmark configuration
	*BenchmarkConfig

	// Optimization settings
	UseOptimizations    bool   `yaml:"use_optimizations"`
	EnableHTTP2         bool   `yaml:"enable_http2"`
	EnableCaching       bool   `yaml:"enable_caching"`
	EnableMonitoring    bool   `yaml:"enable_monitoring"`
	CacheWarmupEnabled  bool   `yaml:"cache_warmup_enabled"`
	ComparisonMode      bool   `yaml:"comparison_mode"`

	// Optimization client
	OptimizedClient     *OptimizedClient `yaml:"-"`
	BaselineClient      *http.Client     `yaml:"-"`
}

// IntegratedBenchmarkResult contains results from both optimized and baseline runs
type IntegratedBenchmarkResult struct {
	// Basic benchmark result
	*BenchmarkResult

	// Optimization metrics
	OptimizationStats   *OptimizationStats `json:"optimization_stats"`

	// Comparison results (if comparison mode enabled)
	ComparisonResult    *ComparisonResult  `json:"comparison_result,omitempty"`

	// Performance targets
	TargetAchievement   *TargetAchievement `json:"target_achievement"`
}

// OptimizationStats contains detailed statistics about optimization performance
type OptimizationStats struct {
	// HTTP/2 performance
	HTTP2Stats struct {
		ConnectionReuse     float64 `json:"connection_reuse_ratio"`
		CompressionRatio    float64 `json:"compression_ratio"`
		StreamUtilization   float64 `json:"stream_utilization"`
	} `json:"http2_stats"`

	// Cache performance
	CacheStats struct {
		HitRatio           float64       `json:"hit_ratio"`
		AverageHitLatency  time.Duration `json:"average_hit_latency"`
		AverageMissLatency time.Duration `json:"average_miss_latency"`
		MemoryUsage        int64         `json:"memory_usage_bytes"`
	} `json:"cache_stats"`

	// Monitoring overhead
	MonitoringStats struct {
		OverheadLatency    time.Duration `json:"overhead_latency"`
		OverheadPercentage float64       `json:"overhead_percentage"`
	} `json:"monitoring_stats"`

	// Overall optimization
	TotalImprovement   float64       `json:"total_improvement_percentage"`
	LatencyReduction   time.Duration `json:"latency_reduction"`
	ThroughputGain     float64       `json:"throughput_gain"`
}

// TargetAchievement tracks whether performance targets were met
type TargetAchievement struct {
	LatencyTarget      bool    `json:"latency_target_met"`
	CacheHitTarget     bool    `json:"cache_hit_target_met"`
	ConnectionTarget   bool    `json:"connection_reuse_target_met"`
	ThroughputTarget   bool    `json:"throughput_target_met"`
	OverallGrade       string  `json:"overall_grade"`
	ScorePercentage    float64 `json:"score_percentage"`
}

// IntegratedBenchmarkEngine extends the benchmark engine with optimization support
type IntegratedBenchmarkEngine struct {
	// Base benchmark engine
	*BenchmarkEngine

	// Optimization components
	optimizer          *IntegratedOptimizer
	optimizedClient    *OptimizedClient
	baselineClient     *http.Client

	// Configuration
	config             *IntegratedBenchmarkConfig

	// State management
	mu                 sync.RWMutex
	running            bool
}

// NewIntegratedBenchmarkEngine creates a new benchmark engine with optimization support
func NewIntegratedBenchmarkEngine(config *IntegratedBenchmarkConfig) (*IntegratedBenchmarkEngine, error) {
	if config == nil {
		return nil, fmt.Errorf("config cannot be nil")
	}

	// Create base benchmark engine
	baseEngine, err := NewBenchmarkEngine(config.BenchmarkConfig)
	if err != nil {
		return nil, fmt.Errorf("failed to create base benchmark engine: %w", err)
	}

	engine := &IntegratedBenchmarkEngine{
		BenchmarkEngine: baseEngine,
		config:         config,
	}

	// Set up optimized client if optimizations are enabled
	if config.UseOptimizations && config.OptimizedClient != nil {
		engine.optimizedClient = config.OptimizedClient
	}

	// Create baseline client for comparisons
	engine.baselineClient = &http.Client{
		Timeout: 30 * time.Second,
	}

	return engine, nil
}

// RunIntegratedBenchmark executes a comprehensive benchmark with all optimizations
func (ibe *IntegratedBenchmarkEngine) RunIntegratedBenchmark(url string) (*IntegratedBenchmarkResult, error) {
	ibe.mu.Lock()
	if ibe.running {
		ibe.mu.Unlock()
		return nil, fmt.Errorf("benchmark already running")
	}
	ibe.running = true
	ibe.mu.Unlock()

	defer func() {
		ibe.mu.Lock()
		ibe.running = false
		ibe.mu.Unlock()
	}()

	log.Printf("Starting integrated benchmark for %s", url)
	startTime := time.Now()

	// Create benchmark run configuration
	runConfig := &BenchmarkRunConfig{
		URL:              url,
		TotalRequests:    ibe.config.TotalRequests,
		Concurrency:      ibe.config.Concurrency,
		Timeout:          ibe.config.RequestTimeout,
		UseOptimizations: ibe.config.UseOptimizations,
	}

	// Run optimized benchmark
	result, err := ibe.runOptimizedBenchmark(runConfig)
	if err != nil {
		return nil, fmt.Errorf("optimized benchmark failed: %w", err)
	}

	// Create integrated result
	integratedResult := &IntegratedBenchmarkResult{
		BenchmarkResult: result,
	}

	// Collect optimization statistics
	if ibe.config.UseOptimizations && ibe.optimizedClient != nil {
		optStats, err := ibe.collectOptimizationStats()
		if err != nil {
			log.Printf("Warning: failed to collect optimization stats: %v", err)
		} else {
			integratedResult.OptimizationStats = optStats
		}
	}

	// Run comparison if enabled
	if ibe.config.ComparisonMode {
		comparison, err := ibe.runComparison(runConfig)
		if err != nil {
			log.Printf("Warning: comparison benchmark failed: %v", err)
		} else {
			integratedResult.ComparisonResult = comparison
		}
	}

	// Evaluate target achievement
	integratedResult.TargetAchievement = ibe.evaluateTargets(integratedResult)

	duration := time.Since(startTime)
	log.Printf("‚úì Integrated benchmark completed in %v", duration)

	return integratedResult, nil
}

// runOptimizedBenchmark executes benchmark using the optimized client
func (ibe *IntegratedBenchmarkEngine) runOptimizedBenchmark(config *BenchmarkRunConfig) (*BenchmarkResult, error) {
	if !ibe.config.UseOptimizations || ibe.optimizedClient == nil {
		// Fall back to standard benchmark
		return ibe.Run(config)
	}

	log.Printf("Running optimized benchmark: %d requests, %d concurrent",
		config.TotalRequests, config.Concurrency)

	// Prepare cache warmup if enabled
	if ibe.config.CacheWarmupEnabled {
		err := ibe.optimizedClient.WarmupCache([]string{config.URL})
		if err != nil {
			log.Printf("Warning: cache warmup failed: %v", err)
		} else {
			log.Println("‚úì Cache warmup completed")
		}
	}

	// Execute benchmark with optimized client
	return ibe.runBenchmarkWithClient(config, ibe.optimizedClient)
}

// runBenchmarkWithClient executes benchmark using a specific client implementation
func (ibe *IntegratedBenchmarkEngine) runBenchmarkWithClient(config *BenchmarkRunConfig, client interface{}) (*BenchmarkResult, error) {
	startTime := time.Now()
	results := make(chan *LatencyMetrics, config.TotalRequests)
	errors := make(chan error, config.TotalRequests)

	// Create semaphore for concurrency control
	semaphore := make(chan struct{}, config.Concurrency)
	var wg sync.WaitGroup

	// Execute requests
	for i := 0; i < config.TotalRequests; i++ {
		wg.Add(1)
		go func(requestID int) {
			defer wg.Done()

			// Acquire semaphore
			semaphore <- struct{}{}
			defer func() { <-semaphore }()

			// Execute request based on client type
			switch c := client.(type) {
			case *OptimizedClient:
				ibe.executeOptimizedRequest(c, config.URL, requestID, results, errors)
			case *http.Client:
				ibe.executeStandardRequest(c, config.URL, requestID, results, errors)
			default:
				errors <- fmt.Errorf("unsupported client type")
			}
		}(i)
	}

	// Wait for completion
	wg.Wait()
	close(results)
	close(errors)

	// Collect results
	var metrics []*LatencyMetrics
	var errorCount int

	// Collect successful results
	for result := range results {
		metrics = append(metrics, result)
	}

	// Count errors
	for range errors {
		errorCount++
	}

	// Generate benchmark result
	return ibe.generateBenchmarkResult(config, metrics, errorCount, startTime, time.Now())
}

// executeOptimizedRequest performs a request using the optimized client
func (ibe *IntegratedBenchmarkEngine) executeOptimizedRequest(client *OptimizedClient, url string, requestID int, results chan<- *LatencyMetrics, errors chan<- error) {
	start := time.Now()

	// Create HTTP request
	req, err := http.NewRequest("GET", url, nil)
	if err != nil {
		errors <- fmt.Errorf("request %d: failed to create request: %w", requestID, err)
		return
	}

	// Create optimized request
	optimizedReq := &OptimizedRequest{
		Request:      req,
		UseCache:     ibe.config.EnableCaching,
		EnableMetrics: ibe.config.EnableMonitoring,
	}

	// Execute request
	resp, err := client.Do(optimizedReq)
	if err != nil {
		errors <- fmt.Errorf("request %d: %w", requestID, err)
		return
	}
	defer resp.Response.Body.Close()

	// Read response body
	body, err := io.ReadAll(resp.Response.Body)
	if err != nil {
		errors <- fmt.Errorf("request %d: failed to read body: %w", requestID, err)
		return
	}

	// Create metrics from optimized response
	metrics := &LatencyMetrics{
		DNSLookup:        resp.DNSLatency,
		TCPConnection:    resp.ConnectLatency,
		TLSHandshake:     resp.TLSLatency,
		ServerProcessing: resp.ProcessingLatency,
		TimeToFirstByte:  resp.TTFBLatency,
		TotalLatency:     resp.TotalLatency,
		StatusCode:       resp.Response.StatusCode,
		ResponseSize:     int64(len(body)),
		Timestamp:        start,
	}

	results <- metrics
}

// executeStandardRequest performs a request using the standard HTTP client
func (ibe *IntegratedBenchmarkEngine) executeStandardRequest(client *http.Client, url string, requestID int, results chan<- *LatencyMetrics, errors chan<- error) {
	// Use the existing benchmark implementation for standard requests
	metrics, err := ibe.BenchmarkEngine.executeSingleRequest(url, requestID)
	if err != nil {
		errors <- err
		return
	}

	results <- metrics
}

// runComparison executes both optimized and baseline benchmarks for comparison
func (ibe *IntegratedBenchmarkEngine) runComparison(config *BenchmarkRunConfig) (*ComparisonResult, error) {
	log.Println("Running comparison benchmark (optimized vs baseline)...")

	// Run optimized benchmark
	optimizedConfig := *config
	optimizedConfig.UseOptimizations = true

	optimizedResult, err := ibe.runBenchmarkWithClient(&optimizedConfig, ibe.optimizedClient)
	if err != nil {
		return nil, fmt.Errorf("optimized benchmark failed: %w", err)
	}

	// Run baseline benchmark
	baselineConfig := *config
	baselineConfig.UseOptimizations = false

	baselineResult, err := ibe.runBenchmarkWithClient(&baselineConfig, ibe.baselineClient)
	if err != nil {
		return nil, fmt.Errorf("baseline benchmark failed: %w", err)
	}

	// Calculate improvements
	comparison := &ComparisonResult{
		Optimized: optimizedResult,
		Baseline:  baselineResult,
		Improvement: &ImprovementMetrics{
			LatencyImprovement:    calculateLatencyImprovement(baselineResult, optimizedResult),
			ThroughputImprovement: calculateThroughputImprovement(baselineResult, optimizedResult),
			EfficiencyScore:       calculateEfficiencyScore(baselineResult, optimizedResult),
		},
	}

	log.Printf("‚úì Comparison completed: %.1f%% latency improvement, %.1f%% throughput improvement",
		comparison.Improvement.LatencyImprovement*100,
		comparison.Improvement.ThroughputImprovement*100,
	)

	return comparison, nil
}

// collectOptimizationStats gathers detailed optimization statistics
func (ibe *IntegratedBenchmarkEngine) collectOptimizationStats() (*OptimizationStats, error) {
	if ibe.optimizedClient == nil {
		return nil, fmt.Errorf("optimized client not available")
	}

	clientStats := ibe.optimizedClient.GetStats()

	stats := &OptimizationStats{
		TotalImprovement: 0, // Will be calculated if comparison is available
	}

	// HTTP/2 statistics
	stats.HTTP2Stats.ConnectionReuse = clientStats.ConnectionReuseRatio
	// Note: Other HTTP/2 stats would need additional metrics from the HTTP/2 client

	// Cache statistics
	stats.CacheStats.HitRatio = clientStats.CacheHitRatio
	// Note: Additional cache metrics would need to be collected from the cache

	// Calculate overall improvement (requires baseline comparison)
	if clientStats.AverageLatency > 0 {
		// Estimate improvement based on cache hit ratio and connection reuse
		cacheImprovement := stats.CacheStats.HitRatio * 0.6 // Assume 60% latency reduction for cache hits
		connectionImprovement := stats.HTTP2Stats.ConnectionReuse * 0.1 // Assume 10% improvement for connection reuse
		stats.TotalImprovement = (cacheImprovement + connectionImprovement) * 100
	}

	return stats, nil
}

// evaluateTargets checks whether performance targets were achieved
func (ibe *IntegratedBenchmarkEngine) evaluateTargets(result *IntegratedBenchmarkResult) *TargetAchievement {
	targets := &TargetAchievement{}

	// Define target values (should be configurable)
	latencyTarget := 100 * time.Millisecond
	cacheHitTarget := 0.6
	connectionReuseTarget := 0.9
	throughputTarget := 50.0 // requests per second

	// Check latency target
	targets.LatencyTarget = result.Latency.P50 <= latencyTarget

	// Check cache hit target
	if result.OptimizationStats != nil {
		targets.CacheHitTarget = result.OptimizationStats.CacheStats.HitRatio >= cacheHitTarget
		targets.ConnectionTarget = result.OptimizationStats.HTTP2Stats.ConnectionReuse >= connectionReuseTarget
	}

	// Check throughput target
	targets.ThroughputTarget = result.Throughput.RequestsPerSecond >= throughputTarget

	// Calculate overall grade
	score := 0
	total := 4

	if targets.LatencyTarget {
		score++
	}
	if targets.CacheHitTarget {
		score++
	}
	if targets.ConnectionTarget {
		score++
	}
	if targets.ThroughputTarget {
		score++
	}

	targets.ScorePercentage = float64(score) / float64(total)

	switch {
	case targets.ScorePercentage >= 0.9:
		targets.OverallGrade = "A"
	case targets.ScorePercentage >= 0.8:
		targets.OverallGrade = "B"
	case targets.ScorePercentage >= 0.7:
		targets.OverallGrade = "C"
	case targets.ScorePercentage >= 0.6:
		targets.OverallGrade = "D"
	default:
		targets.OverallGrade = "F"
	}

	return targets
}

// SaveIntegratedResults saves the integrated benchmark results to a file
func (ibe *IntegratedBenchmarkEngine) SaveIntegratedResults(result *IntegratedBenchmarkResult, filename string) error {
	file, err := os.Create(filename)
	if err != nil {
		return fmt.Errorf("failed to create file: %w", err)
	}
	defer file.Close()

	encoder := json.NewEncoder(file)
	encoder.SetIndent("", "  ")

	if err := encoder.Encode(result); err != nil {
		return fmt.Errorf("failed to encode result: %w", err)
	}

	log.Printf("‚úì Integrated results saved to %s", filename)
	return nil
}

// GenerateIntegratedReport creates a comprehensive performance report
func (ibe *IntegratedBenchmarkEngine) GenerateIntegratedReport(result *IntegratedBenchmarkResult) string {
	report := fmt.Sprintf(`
# API Latency Optimization - Phase 1 Results

## Test Configuration
- Target URL: %s
- Total Requests: %d
- Concurrency: %d
- Duration: %v
- Optimizations Enabled: %t

## Performance Metrics
- **P50 Latency**: %v
- **P95 Latency**: %v
- **P99 Latency**: %v
- **Average Latency**: %v
- **Throughput**: %.2f req/s
- **Success Rate**: %.2f%%

## Optimization Performance
`,
		result.TargetURL,
		result.TotalRequests,
		result.Concurrency,
		result.Duration,
		ibe.config.UseOptimizations,
		result.Latency.P50,
		result.Latency.P95,
		result.Latency.P99,
		result.Latency.Average,
		result.Throughput.RequestsPerSecond,
		result.SuccessRate,
	)

	if result.OptimizationStats != nil {
		report += fmt.Sprintf(`
### HTTP/2 Performance
- **Connection Reuse**: %.1f%%
- **Compression**: Enabled

### Cache Performance
- **Hit Ratio**: %.1f%%
- **Total Improvement**: %.1f%%

`,
			result.OptimizationStats.HTTP2Stats.ConnectionReuse*100,
			result.OptimizationStats.CacheStats.HitRatio*100,
			result.OptimizationStats.TotalImprovement,
		)
	}

	if result.ComparisonResult != nil {
		report += fmt.Sprintf(`
## Optimization Impact
- **Latency Improvement**: %.1f%% (%.2fms ‚Üí %.2fms)
- **Throughput Improvement**: %.1f%% (%.2f ‚Üí %.2f req/s)
- **Efficiency Score**: %.3f

`,
			result.ComparisonResult.Improvement.LatencyImprovement*100,
			float64(result.ComparisonResult.Baseline.Latency.P50.Nanoseconds())/1000000,
			float64(result.ComparisonResult.Optimized.Latency.P50.Nanoseconds())/1000000,
			result.ComparisonResult.Improvement.ThroughputImprovement*100,
			result.ComparisonResult.Baseline.Throughput.RequestsPerSecond,
			result.ComparisonResult.Optimized.Throughput.RequestsPerSecond,
			result.ComparisonResult.Improvement.EfficiencyScore,
		)
	}

	if result.TargetAchievement != nil {
		report += fmt.Sprintf(`
## Target Achievement
- **Overall Grade**: %s (%.0f%%)
- **Latency Target**: %t (< 100ms)
- **Cache Hit Target**: %t (> 60%%)
- **Connection Reuse Target**: %t (> 90%%)
- **Throughput Target**: %t (> 50 req/s)

`,
			result.TargetAchievement.OverallGrade,
			result.TargetAchievement.ScorePercentage*100,
			result.TargetAchievement.LatencyTarget,
			result.TargetAchievement.CacheHitTarget,
			result.TargetAchievement.ConnectionTarget,
			result.TargetAchievement.ThroughputTarget,
		)
	}

	report += fmt.Sprintf(`
## Phase 1 Status
%s

*Generated at: %v*
`, ibe.getPhase1Status(result), time.Now().Format(time.RFC3339))

	return report
}

// getPhase1Status determines the overall Phase 1 completion status
func (ibe *IntegratedBenchmarkEngine) getPhase1Status(result *IntegratedBenchmarkResult) string {
	if result.TargetAchievement == nil {
		return "‚ùì Status unknown - unable to evaluate targets"
	}

	switch result.TargetAchievement.OverallGrade {
	case "A":
		return "üéâ **PHASE 1 COMPLETE** - All targets exceeded! Ready for Phase 2."
	case "B":
		return "‚úÖ **PHASE 1 SUCCESS** - Major targets achieved. Minor optimizations recommended."
	case "C":
		return "‚ö†Ô∏è **PHASE 1 PARTIAL** - Some targets achieved. Additional optimization needed."
	case "D":
		return "‚ùå **PHASE 1 INCOMPLETE** - Significant issues identified. Review implementation."
	case "F":
		return "üö® **PHASE 1 FAILED** - Critical issues. Requires immediate attention."
	default:
		return "‚ùì **PHASE 1 UNKNOWN** - Unable to determine status."
	}
}

// IsRunning returns true if a benchmark is currently running
func (ibe *IntegratedBenchmarkEngine) IsRunning() bool {
	ibe.mu.RLock()
	defer ibe.mu.RUnlock()
	return ibe.running
}